{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on BaSalam Reviews\n",
    "This notebook demonstrates the process of downloading, preprocessing, and analyzing sentiment on the BaSalam reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import kagglehub\n",
    "from sklearn.utils import resample\n",
    "from sentiment_analysis import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "Download the latest version of the BaSalam reviews dataset using `kagglehub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"radeai/basalam-comments-and-products\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Load the reviews dataset into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(f'{path}/BaSalam.reviews.csv', low_memory=False)\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "Define functions to identify stickers and preprocess comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sticker(token):\n",
    "    \"\"\"\n",
    "    Checks if a given token is a sticker.\n",
    "\n",
    "    A token is considered a sticker if it meets any of the following criteria:\n",
    "    1. It is an image file with extensions .webp, .png, .gif, or .jpg.\n",
    "    2. It is an emoji.\n",
    "    3. It is a URL.\n",
    "\n",
    "    Parameters:\n",
    "    token (str): The input token to be checked.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the token is a sticker, False otherwise.\n",
    "    \"\"\"\n",
    "    if re.match(r'.*\\.(webp|png|gif|jpg)$', token):\n",
    "        return True\n",
    "    if emoji.is_emoji(token):\n",
    "        return True\n",
    "    if re.match(r'https?://[^\\s]+', token):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(comment):\n",
    "    \"\"\"\n",
    "    Preprocesses a given comment by performing the following steps:\n",
    "    1. Replaces all emojis with a space.\n",
    "    2. Removes URLs.\n",
    "    3. Removes all non-word characters (punctuation).\n",
    "    4. Removes all digits.\n",
    "\n",
    "    Parameters:\n",
    "    comment (str): The input comment to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed comment.\n",
    "    \"\"\"\n",
    "    comment = emoji.replace_emoji(comment, replace=\" \")\n",
    "    comment = re.sub(r'https?://\\S+|www\\.\\S+', ' ', comment)\n",
    "    comment = re.sub(r'[^\\w\\s]', ' ', comment)\n",
    "    comment = re.sub(r'\\d+', ' ', comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Reviews\n",
    "Filter and preprocess the reviews dataset to create a description dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and preprocess the reviews dataset to create a description dataframe\n",
    "\n",
    "description = reviews[reviews['description'].notna()][['_id', 'productId', 'star', 'description']]\n",
    "print(description.shape)\n",
    "print(description.drop_duplicates(subset=['description']).shape)\n",
    "description = description.drop_duplicates(subset=['description'])\n",
    "description['preprocessed_description'] = description['description'].apply(preprocessing)\n",
    "print(description.shape)\n",
    "print(description.drop_duplicates(subset=['preprocessed_description']).shape)\n",
    "description = description.drop_duplicates(subset=['preprocessed_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Dataset\n",
    "Balance the dataset by resampling each star rating group to have the same number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset by resampling each star rating group to have the same number of samples\n",
    "\n",
    "star_groups = []\n",
    "for star in description['star'].unique():\n",
    "    star_groups.append(description[description['star'] == star])\n",
    "\n",
    "min_samples = min([len(group) for group in star_groups])\n",
    "\n",
    "balanced_samples = []\n",
    "for group in star_groups:\n",
    "    balanced_samples.append(resample(group, replace=False, n_samples=min(min_samples, 5000), random_state=42))\n",
    "\n",
    "balanced_description = pd.concat(balanced_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Balanced Dataset\n",
    "Check the distribution of star ratings in the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_description['star'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Balanced Dataset\n",
    "Save the balanced dataset to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_description.to_csv('sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Process the CSV file in chunks, perform sentiment analysis on each chunk, and save the results to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part processes a CSV file in chunks, performs sentiment analysis on each chunk,\n",
    "and saves the results to a new CSV file. It uses the SentimentAnalyzer class from the\n",
    "sentiment_analysis module to classify the sentiment of each description in the dataset.\n",
    "\"\"\"\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "chunk_size = 100\n",
    "batch_size = 5\n",
    "\n",
    "input_file = \"sample.csv\"\n",
    "output_file = \"sentiment_results.csv\"\n",
    "\n",
    "chunk_number = 0\n",
    "\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as f_out:\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "        chunk_number += 1\n",
    "        sentiments = []\n",
    "\n",
    "        print(f\"Processing Chunk {chunk_number}...\")\n",
    "\n",
    "        for i in range(0, len(chunk), batch_size):\n",
    "            batch = chunk.iloc[i:i + batch_size]\n",
    "            descriptions = batch['preprocessed_description'].tolist()\n",
    "\n",
    "            if descriptions:\n",
    "                batch_results = analyzer.classify(descriptions, method='batch')\n",
    "\n",
    "                if len(batch_results) == len(batch):\n",
    "                    sentiments.extend(batch_results)\n",
    "                else:\n",
    "                    print(f\"⚠️ Warning: Mismatch in batch size at Chunk {chunk_number}, Batch {i // batch_size + 1}\")\n",
    "                    sentiments.extend([\"error\"] * len(batch))\n",
    "\n",
    "            batch_number = (i // batch_size) + 1\n",
    "            print(f\"   Processing Batch {batch_number} in Chunk {chunk_number}\")\n",
    "\n",
    "        if len(sentiments) == len(chunk):\n",
    "            chunk[\"sentiment\"] = sentiments\n",
    "        else:\n",
    "            print(f\"⚠️ Error: Sentiments list ({len(sentiments)}) does not match chunk size ({len(chunk)})\")\n",
    "            chunk[\"sentiment\"] = [\"error\"] * len(chunk)\n",
    "\n",
    "        chunk.to_csv(f_out, mode='a', index=False, header=f_out.tell() == 0)\n",
    "\n",
    "        print(f\"✅ Finished Chunk {chunk_number}, saved results to {output_file}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
